{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6ac48658-c6a6-413e-8756-422153d887d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to C:\\Users\\Gaurang\n",
      "[nltk_data]     Vaghela\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Gaurang\n",
      "[nltk_data]     Vaghela\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Gaurang\n",
      "[nltk_data]     Vaghela\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\Gaurang Vaghela\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "575b9f8d-6b89-4b86-ab0e-affffc989e23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One of the most frequent types of day-to-day conversion is text communication. In our everyday routine, we chat, message, tweet, share status, email, create blogs, and offer opinions and criticism. All of these actions lead to a substantial amount of unstructured text being produced. It is critical to examine huge amounts of data in this sector of the online world and social media to determine people's opinions. Text mining is also referred to as text analytics. Text mining is a process of exploring sizable textual data and finding patterns. Text Mining processes the text itself, while NLP processes with the underlying metadata. Finding frequency counts of words, length of the sentence, presence/absence of specific words is known as text mining. Natural language processing is one of the components of text mining. NLP helps identify sentiment, finding entities in the sentence, and category of blog/article. Text mining is preprocessed data for text analytics. In Text Analytics, statistical and machine learning algorithms are used to classify information.\n",
      "\n",
      "\n",
      "['One of the most frequent types of day-to-day conversion is text communication.', 'In our everyday routine, we chat, message, tweet, share status, email, create blogs, and offer opinions and criticism.', 'All of these actions lead to a substantial amount of unstructured text being produced.', \"It is critical to examine huge amounts of data in this sector of the online world and social media to determine people's opinions.\", 'Text mining is also referred to as text analytics.', 'Text mining is a process of exploring sizable textual data and finding patterns.', 'Text Mining processes the text itself, while NLP processes with the underlying metadata.', 'Finding frequency counts of words, length of the sentence, presence/absence of specific words is known as text mining.', 'Natural language processing is one of the components of text mining.', 'NLP helps identify sentiment, finding entities in the sentence, and category of blog/article.', 'Text mining is preprocessed data for text analytics.', 'In Text Analytics, statistical and machine learning algorithms are used to classify information.']\n",
      "\n",
      "\n",
      "['One', 'of', 'the', 'most', 'frequent', 'types', 'of', 'day-to-day', 'conversion', 'is', 'text', 'communication', '.', 'In', 'our', 'everyday', 'routine', ',', 'we', 'chat', ',', 'message', ',', 'tweet', ',', 'share', 'status', ',', 'email', ',', 'create', 'blogs', ',', 'and', 'offer', 'opinions', 'and', 'criticism', '.', 'All', 'of', 'these', 'actions', 'lead', 'to', 'a', 'substantial', 'amount', 'of', 'unstructured', 'text', 'being', 'produced', '.', 'It', 'is', 'critical', 'to', 'examine', 'huge', 'amounts', 'of', 'data', 'in', 'this', 'sector', 'of', 'the', 'online', 'world', 'and', 'social', 'media', 'to', 'determine', 'people', \"'s\", 'opinions', '.', 'Text', 'mining', 'is', 'also', 'referred', 'to', 'as', 'text', 'analytics', '.', 'Text', 'mining', 'is', 'a', 'process', 'of', 'exploring', 'sizable', 'textual', 'data', 'and', 'finding', 'patterns', '.', 'Text', 'Mining', 'processes', 'the', 'text', 'itself', ',', 'while', 'NLP', 'processes', 'with', 'the', 'underlying', 'metadata', '.', 'Finding', 'frequency', 'counts', 'of', 'words', ',', 'length', 'of', 'the', 'sentence', ',', 'presence/absence', 'of', 'specific', 'words', 'is', 'known', 'as', 'text', 'mining', '.', 'Natural', 'language', 'processing', 'is', 'one', 'of', 'the', 'components', 'of', 'text', 'mining', '.', 'NLP', 'helps', 'identify', 'sentiment', ',', 'finding', 'entities', 'in', 'the', 'sentence', ',', 'and', 'category', 'of', 'blog/article', '.', 'Text', 'mining', 'is', 'preprocessed', 'data', 'for', 'text', 'analytics', '.', 'In', 'Text', 'Analytics', ',', 'statistical', 'and', 'machine', 'learning', 'algorithms', 'are', 'used', 'to', 'classify', 'information', '.']\n"
     ]
    }
   ],
   "source": [
    "file = open(\"C:/Users/Gaurang Vaghela/OneDrive/Desktop/Dataset/Text.txt\",'r')\n",
    "text = file.read()\n",
    "\n",
    "print(text)\n",
    "print('\\n')\n",
    "\n",
    "tokens_sent = nltk.sent_tokenize(text)\n",
    "print(tokens_sent)\n",
    "print('\\n')\n",
    "\n",
    "tokens_words = nltk.word_tokenize(text)\n",
    "print(tokens_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d24139f6-d972-4627-8c56-198a9b94b94d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('One', 'CD'), ('of', 'IN'), ('the', 'DT'), ('most', 'RBS'), ('frequent', 'JJ'), ('types', 'NNS'), ('of', 'IN'), ('day-to-day', 'JJ'), ('conversion', 'NN'), ('is', 'VBZ'), ('text', 'JJ'), ('communication', 'NN'), ('.', '.'), ('In', 'IN'), ('our', 'PRP$'), ('everyday', 'JJ'), ('routine', 'JJ'), (',', ','), ('we', 'PRP'), ('chat', 'VBP'), (',', ','), ('message', 'NN'), (',', ','), ('tweet', 'NN'), (',', ','), ('share', 'NN'), ('status', 'NN'), (',', ','), ('email', 'NN'), (',', ','), ('create', 'JJ'), ('blogs', 'NNS'), (',', ','), ('and', 'CC'), ('offer', 'VBP'), ('opinions', 'NNS'), ('and', 'CC'), ('criticism', 'NN'), ('.', '.'), ('All', 'DT'), ('of', 'IN'), ('these', 'DT'), ('actions', 'NNS'), ('lead', 'VBP'), ('to', 'TO'), ('a', 'DT'), ('substantial', 'JJ'), ('amount', 'NN'), ('of', 'IN'), ('unstructured', 'JJ'), ('text', 'NN'), ('being', 'VBG'), ('produced', 'VBN'), ('.', '.'), ('It', 'PRP'), ('is', 'VBZ'), ('critical', 'JJ'), ('to', 'TO'), ('examine', 'VB'), ('huge', 'JJ'), ('amounts', 'NNS'), ('of', 'IN'), ('data', 'NNS'), ('in', 'IN'), ('this', 'DT'), ('sector', 'NN'), ('of', 'IN'), ('the', 'DT'), ('online', 'JJ'), ('world', 'NN'), ('and', 'CC'), ('social', 'JJ'), ('media', 'NNS'), ('to', 'TO'), ('determine', 'VB'), ('people', 'NNS'), (\"'s\", 'POS'), ('opinions', 'NNS'), ('.', '.'), ('Text', 'NNP'), ('mining', 'NN'), ('is', 'VBZ'), ('also', 'RB'), ('referred', 'VBN'), ('to', 'TO'), ('as', 'IN'), ('text', 'JJ'), ('analytics', 'NNS'), ('.', '.'), ('Text', 'NNP'), ('mining', 'NN'), ('is', 'VBZ'), ('a', 'DT'), ('process', 'NN'), ('of', 'IN'), ('exploring', 'VBG'), ('sizable', 'JJ'), ('textual', 'JJ'), ('data', 'NNS'), ('and', 'CC'), ('finding', 'VBG'), ('patterns', 'NNS'), ('.', '.'), ('Text', 'NNP'), ('Mining', 'NNP'), ('processes', 'VBZ'), ('the', 'DT'), ('text', 'NN'), ('itself', 'PRP'), (',', ','), ('while', 'IN'), ('NLP', 'NNP'), ('processes', 'VBZ'), ('with', 'IN'), ('the', 'DT'), ('underlying', 'VBG'), ('metadata', 'NN'), ('.', '.'), ('Finding', 'VBG'), ('frequency', 'NN'), ('counts', 'NNS'), ('of', 'IN'), ('words', 'NNS'), (',', ','), ('length', 'NN'), ('of', 'IN'), ('the', 'DT'), ('sentence', 'NN'), (',', ','), ('presence/absence', 'NN'), ('of', 'IN'), ('specific', 'JJ'), ('words', 'NNS'), ('is', 'VBZ'), ('known', 'VBN'), ('as', 'IN'), ('text', 'NN'), ('mining', 'NN'), ('.', '.'), ('Natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('is', 'VBZ'), ('one', 'CD'), ('of', 'IN'), ('the', 'DT'), ('components', 'NNS'), ('of', 'IN'), ('text', 'NN'), ('mining', 'NN'), ('.', '.'), ('NLP', 'NNP'), ('helps', 'VBZ'), ('identify', 'VB'), ('sentiment', 'NN'), (',', ','), ('finding', 'VBG'), ('entities', 'NNS'), ('in', 'IN'), ('the', 'DT'), ('sentence', 'NN'), (',', ','), ('and', 'CC'), ('category', 'NN'), ('of', 'IN'), ('blog/article', 'NN'), ('.', '.'), ('Text', 'NNP'), ('mining', 'NN'), ('is', 'VBZ'), ('preprocessed', 'VBN'), ('data', 'NNS'), ('for', 'IN'), ('text', 'NN'), ('analytics', 'NNS'), ('.', '.'), ('In', 'IN'), ('Text', 'NNP'), ('Analytics', 'NNP'), (',', ','), ('statistical', 'JJ'), ('and', 'CC'), ('machine', 'NN'), ('learning', 'NN'), ('algorithms', 'NN'), ('are', 'VBP'), ('used', 'VBN'), ('to', 'TO'), ('classify', 'VB'), ('information', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "tagged = pos_tag(tokens_words)\n",
    "print(tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bac50110-bf34-4f98-9d63-351d2166f1c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Tokens (Stopwords Removed): ['One', 'frequent', 'types', 'day-to-day', 'conversion', 'text', 'communication', 'everyday', 'routine', 'chat', 'message', 'tweet', 'share', 'status', 'email', 'create', 'blogs', 'offer', 'opinions', 'criticism', 'actions', 'lead', 'substantial', 'amount', 'unstructured', 'text', 'produced', 'critical', 'examine', 'huge', 'amounts', 'data', 'sector', 'online', 'world', 'social', 'media', 'determine', 'people', \"'s\", 'opinions', 'Text', 'mining', 'also', 'referred', 'text', 'analytics', 'Text', 'mining', 'process', 'exploring', 'sizable', 'textual', 'data', 'finding', 'patterns', 'Text', 'Mining', 'processes', 'text', 'NLP', 'processes', 'underlying', 'metadata', 'Finding', 'frequency', 'counts', 'words', 'length', 'sentence', 'presence/absence', 'specific', 'words', 'known', 'text', 'mining', 'Natural', 'language', 'processing', 'one', 'components', 'text', 'mining', 'NLP', 'helps', 'identify', 'sentiment', 'finding', 'entities', 'sentence', 'category', 'blog/article', 'Text', 'mining', 'preprocessed', 'data', 'text', 'analytics', 'Text', 'Analytics', 'statistical', 'machine', 'learning', 'algorithms', 'used', 'classify', 'information']\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [word for word in tokens_words if word.lower() not in stop_words and word not in string.punctuation]\n",
    "print(\"Filtered Tokens (Stopwords Removed):\", filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4a9bae83-aa50-46c4-84a0-648c8afd9cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed Tokens: ['one', 'frequent', 'type', 'day-to-day', 'convers', 'text', 'commun', 'everyday', 'routin', 'chat', 'messag', 'tweet', 'share', 'statu', 'email', 'creat', 'blog', 'offer', 'opinion', 'critic', 'action', 'lead', 'substanti', 'amount', 'unstructur', 'text', 'produc', 'critic', 'examin', 'huge', 'amount', 'data', 'sector', 'onlin', 'world', 'social', 'media', 'determin', 'peopl', \"'s\", 'opinion', 'text', 'mine', 'also', 'refer', 'text', 'analyt', 'text', 'mine', 'process', 'explor', 'sizabl', 'textual', 'data', 'find', 'pattern', 'text', 'mine', 'process', 'text', 'nlp', 'process', 'underli', 'metadata', 'find', 'frequenc', 'count', 'word', 'length', 'sentenc', 'presence/abs', 'specif', 'word', 'known', 'text', 'mine', 'natur', 'languag', 'process', 'one', 'compon', 'text', 'mine', 'nlp', 'help', 'identifi', 'sentiment', 'find', 'entiti', 'sentenc', 'categori', 'blog/articl', 'text', 'mine', 'preprocess', 'data', 'text', 'analyt', 'text', 'analyt', 'statist', 'machin', 'learn', 'algorithm', 'use', 'classifi', 'inform']\n"
     ]
    }
   ],
   "source": [
    "stemmer = PorterStemmer()\n",
    "stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]\n",
    "print(\"Stemmed Tokens:\", stemmed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ac5a73ca-13e1-442a-88e3-c5c9f770e2f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized Tokens: ['One', 'frequent', 'type', 'day-to-day', 'conversion', 'text', 'communication', 'everyday', 'routine', 'chat', 'message', 'tweet', 'share', 'status', 'email', 'create', 'blog', 'offer', 'opinion', 'criticism', 'action', 'lead', 'substantial', 'amount', 'unstructured', 'text', 'produced', 'critical', 'examine', 'huge', 'amount', 'data', 'sector', 'online', 'world', 'social', 'medium', 'determine', 'people', \"'s\", 'opinion', 'Text', 'mining', 'also', 'referred', 'text', 'analytics', 'Text', 'mining', 'process', 'exploring', 'sizable', 'textual', 'data', 'finding', 'pattern', 'Text', 'Mining', 'process', 'text', 'NLP', 'process', 'underlying', 'metadata', 'Finding', 'frequency', 'count', 'word', 'length', 'sentence', 'presence/absence', 'specific', 'word', 'known', 'text', 'mining', 'Natural', 'language', 'processing', 'one', 'component', 'text', 'mining', 'NLP', 'help', 'identify', 'sentiment', 'finding', 'entity', 'sentence', 'category', 'blog/article', 'Text', 'mining', 'preprocessed', 'data', 'text', 'analytics', 'Text', 'Analytics', 'statistical', 'machine', 'learning', 'algorithm', 'used', 'classify', 'information']\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
    "print(\"Lemmatized Tokens:\", lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "106546b0-8f98-467a-ab55-ae013482fe06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TF-IDF Representation:\n",
      "      absence   actions  algorithms       all      also    amount   amounts  \\\n",
      "0   0.000000  0.000000    0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "1   0.000000  0.000000    0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "2   0.000000  0.306501    0.000000  0.306501  0.000000  0.306501  0.000000   \n",
      "3   0.000000  0.000000    0.000000  0.000000  0.000000  0.000000  0.233516   \n",
      "4   0.000000  0.000000    0.000000  0.000000  0.444878  0.000000  0.000000   \n",
      "5   0.000000  0.000000    0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "6   0.000000  0.000000    0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "7   0.245772  0.000000    0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "8   0.000000  0.000000    0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "9   0.000000  0.000000    0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "10  0.000000  0.000000    0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "11  0.000000  0.000000    0.316308  0.000000  0.000000  0.000000  0.000000   \n",
      "\n",
      "    analytics       and       are  ...     tweet     types  underlying  \\\n",
      "0    0.000000  0.000000  0.000000  ...  0.000000  0.289064    0.000000   \n",
      "1    0.000000  0.301948  0.000000  ...  0.244513  0.000000    0.000000   \n",
      "2    0.000000  0.000000  0.000000  ...  0.000000  0.000000    0.000000   \n",
      "3    0.000000  0.144184  0.000000  ...  0.000000  0.000000    0.000000   \n",
      "4    0.337501  0.000000  0.000000  ...  0.000000  0.000000    0.000000   \n",
      "5    0.000000  0.224289  0.000000  ...  0.000000  0.000000    0.000000   \n",
      "6    0.000000  0.000000  0.000000  ...  0.000000  0.000000    0.287484   \n",
      "7    0.000000  0.000000  0.000000  ...  0.000000  0.000000    0.000000   \n",
      "8    0.000000  0.000000  0.000000  ...  0.000000  0.000000    0.000000   \n",
      "9    0.000000  0.190722  0.000000  ...  0.000000  0.000000    0.000000   \n",
      "10   0.357253  0.000000  0.000000  ...  0.000000  0.000000    0.000000   \n",
      "11   0.239963  0.195304  0.316308  ...  0.000000  0.000000    0.000000   \n",
      "\n",
      "    unstructured      used        we     while      with     words     world  \n",
      "0       0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
      "1       0.000000  0.000000  0.244513  0.000000  0.000000  0.000000  0.000000  \n",
      "2       0.306501  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
      "3       0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.233516  \n",
      "4       0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
      "5       0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
      "6       0.000000  0.000000  0.000000  0.287484  0.287484  0.000000  0.000000  \n",
      "7       0.000000  0.000000  0.000000  0.000000  0.000000  0.491543  0.000000  \n",
      "8       0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
      "9       0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
      "10      0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
      "11      0.000000  0.316308  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
      "\n",
      "[12 rows x 100 columns]\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(tokens_sent)\n",
    "\n",
    "# Convert TF-IDF matrix to DataFrame for better visualization\n",
    "df_tfidf = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "print(\"\\nTF-IDF Representation:\\n\", df_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d15f50a-48b9-4ebf-a198-db42095ba54a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
